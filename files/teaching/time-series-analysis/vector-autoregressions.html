<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Natasha Kang">

<title>Vector Autoregressions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="unit7_files/libs/clipboard/clipboard.min.js"></script>
<script src="unit7_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="unit7_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="unit7_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="unit7_files/libs/quarto-html/popper.min.js"></script>
<script src="unit7_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="unit7_files/libs/quarto-html/anchor.min.js"></script>
<link href="unit7_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="unit7_files/libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="unit7_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="unit7_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="unit7_files/libs/bootstrap/bootstrap-d6a003b94517c951b2d65075d42fb01b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="vector-autoregressions.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Vector Autoregressions</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Natasha Kang </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Xiamen University, Chow Institute
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="recall-adl-models" class="level2">
<h2 class="anchored" data-anchor-id="recall-adl-models">Recall: ADL Models</h2>
<p>We have studied dynamic relationships using autoregressive distributed lag (ADL) models.</p>
<p><span class="math display">\[
y_t
=
\alpha
+
\sum_{j=1}^p \phi_j y_{t-j}
+
\sum_{k=0}^q \beta_k x_{t-k}
+
u_t.
\]</span></p>
<p>The model captures:</p>
<ul>
<li>persistence (lags of <span class="math inline">\(y_t\)</span>)</li>
<li>dynamic effects of <span class="math inline">\(x_t\)</span> through its lags</li>
<li>gradual adjustment over time</li>
</ul>
<p>But they rely on an important <strong>modeling assumption</strong>.</p>
<p><br></p>
<div class="fragment">
<p>For consistent estimation and interpretation:</p>
<ul>
<li>the regressors must be weakly exogenous</li>
<li>feedback from <span class="math inline">\(y_t\)</span> to <span class="math inline">\(x_t\)</span> is ruled out</li>
</ul>
<p>This assumption is often implicit, but it is essential. In many macroeconomic settings, however:</p>
<ul>
<li>policy reacts to economic conditions</li>
<li>variables evolve jointly</li>
<li>feedback is pervasive</li>
</ul>
</div>
</section>
<section id="a-simple-control-system-analogy" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-control-system-analogy">A Simple Control-System Analogy</h2>
<p>Consider a thermostat regulating room temperature:</p>
<ul>
<li><span class="math inline">\(y_t\)</span>: room temperature</li>
<li><span class="math inline">\(x_t\)</span>: thermostat setting</li>
<li><span class="math inline">\(u_t\)</span>: unobserved temperature shocks (weather, insulation, people entering the room)</li>
</ul>
<p>The thermostat follows a rule: it adjusts the setting in response to past temperature outcomes.</p>
<p>For example, <span class="math display">\[
x_t = \gamma (y_{t-1} - y^\ast),
\]</span> where <span class="math inline">\(y^\ast\)</span> is the target temperature.</p>
<p><br></p>
<div class="fragment">
<p>Room temperature evolves as: <span class="math display">\[
y_t = \alpha y_{t-1} + \beta x_t + u_t.
\]</span></p>
<p>Because <span class="math inline">\(x_t\)</span> is constructed as a function of past outcomes, and past outcomes depend on past shocks, we have:</p>
<p><span class="math display">\[
\mathrm{Cov}(x_t, u_{t-1}) \neq 0.
\]</span></p>
<p>Exogeneity fails <strong>because</strong> policy responds to the state of the system.</p>
</div>
</section>
<section id="large-scale-macroeconometric-models" class="level2">
<h2 class="anchored" data-anchor-id="large-scale-macroeconometric-models">Large-Scale Macroeconometric Models</h2>
<p>Historically, macroeconomists modeled the economy using large-scale macroeconometric models.</p>
<p>These models:</p>
<ul>
<li>consist of many simultaneous equations</li>
<li>specify behavioral relationships for each variable (e.g.&nbsp;consumption, investment, etc.)</li>
<li>explicitly allow feedback across the system</li>
</ul>
<p><br></p>
<div class="fragment">
<p>For example, a model might include equations such as: <span class="math display">\[
\begin{aligned}
C_t &amp;= f(Y_t, T_t, W_{t-1}, C_{t-1}) \\
I_t &amp;= g(Y_t, r_t, I_{t-1}) \\
M_t &amp;= h(Y_t, P_t, M_{t-1})
\end{aligned}
\]</span></p>
</div>
<p><br></p>
<div class="fragment">
<p>This structure creates several difficulties:</p>
<ul>
<li>if a variable appears on the right-hand side of one equation (e.g.&nbsp;<span class="math inline">\(r_t\)</span> in the investment equation), it is unclear why it is excluded from others</li>
<li>differences in regressors across equations are not disciplined by economic theory, but imposed through ad hoc modeling choices</li>
<li>exclusion and lag restrictions that appear reasonable within individual equations (e.g.&nbsp;allowing <span class="math inline">\(r_t\)</span> to affect <span class="math inline">\(I_t\)</span> but not <span class="math inline">\(C_t\)</span>) imply strong system-wide restrictions once the equations are linked</li>
</ul>
<!-- As Sims (1980) emphasized, such asymmetries are difficult to justify
once variables are viewed as part of an interacting system. -->
</div>
</section>
<section id="symmetric-dynamic-system" class="level2">
<h2 class="anchored" data-anchor-id="symmetric-dynamic-system">Symmetric Dynamic System</h2>
<p>When it is not clear which variables are exogenous, a natural modeling choice is to treat all variables symmetrically.</p>
<p>For example, consider a simple bivariate system</p>
<p><span class="math display">\[
\begin{aligned}
y_t &amp;= b_{10} - b_{12} z_t
      + \gamma_{11} y_{t-1}
      + \gamma_{12} z_{t-1}
      + \varepsilon_{yt}, \\
z_t &amp;= b_{20} - b_{21} y_t
      + \gamma_{21} y_{t-1}
      + \gamma_{22} z_{t-1}
      + \varepsilon_{zt}.
\end{aligned}
\]</span></p>
<div class="fragment">
<p><br></p>
<p>Let <span class="math display">\[
x_t =
\begin{bmatrix}
y_t \\ z_t
\end{bmatrix},
\qquad
\varepsilon_t =
\begin{bmatrix}
\varepsilon_{yt} \\ \varepsilon_{zt}
\end{bmatrix}.
\]</span></p>
<p>The system can be written compactly as <span class="math display">\[
B x_t = \Gamma_0 + \Gamma_1 x_{t-1} + \varepsilon_t,
\]</span> where</p>
<p><span class="math display">\[
B =
\begin{bmatrix}
1 &amp; b_{12} \\
b_{21} &amp; 1
\end{bmatrix},
\qquad
\Gamma_0 =
\begin{bmatrix}
b_{10} \\
b_{20}
\end{bmatrix},
\qquad
\Gamma_1 =
\begin{bmatrix}
\gamma_{11} &amp; \gamma_{12} \\
\gamma_{21} &amp; \gamma_{22}
\end{bmatrix}.
\]</span></p>
</div>
</section>
<section id="structural-restrictions-and-reduced-form" class="level2">
<h2 class="anchored" data-anchor-id="structural-restrictions-and-reduced-form">Structural Restrictions and Reduced Form</h2>
<p>The matrix <span class="math inline">\(B\)</span> summarizes <strong>contemporaneous structural restrictions</strong>.</p>
<p><br></p>
<p>If <span class="math inline">\(B\)</span> is invertible, the system implies a reduced-form representation: <span class="math display">\[
x_t = A_0 + A_1 x_{t-1} + u_t,
\qquad
u_t = B^{-1}\varepsilon_t.
\]</span></p>
<p>The reduced form is <strong>invariant</strong> to the choice of contemporaneous structural restrictions, and can be estimated by OLS.</p>
<p><br></p>
<div class="fragment">
<blockquote class="blockquote">
<p>Sims (1980) proposed estimating this reduced form directly, arguing that many structural models relied on “incredible restrictions.”</p>
</blockquote>
</div>
</section>
<section id="stability-and-stationarity" class="level2">
<h2 class="anchored" data-anchor-id="stability-and-stationarity">Stability and Stationarity</h2>
<p>Consider the VAR(1): <span class="math display">\[
x_t = A_0 + A_1 x_{t-1} + u_t.
\]</span></p>
<p>Iterating backward, <span class="math display">\[
x_t
=
\Big(\sum_{i=0}^{k} A_1^i\Big) A_0
+
\sum_{i=0}^{k} A_1^i u_{t-i}
+
A_1^{k+1} x_{t-k-1}.
\]</span></p>
<p><strong>Stability condition:</strong> all eigenvalues of <span class="math inline">\(A_1\)</span> lie strictly inside the unit circle.</p>
<p><br></p>
<div class="fragment">
<section id="consequences-of-stability" class="level3">
<h3 class="anchored" data-anchor-id="consequences-of-stability">Consequences of Stability</h3>
<p>If all eigenvalues of <span class="math inline">\(A_1\)</span> lie strictly inside the unit circle, then:</p>
<ul>
<li><span class="math inline">\(A_1^{k+1} x_{t-k-1} \to 0\)</span> as <span class="math inline">\(k \to \infty\)</span> (the effect of initial conditions vanishes)</li>
<li>The series <span class="math inline">\(\sum_{i=0}^{\infty} A_1^i u_{t-i}\)</span> converges in mean square</li>
<li><span class="math inline">\(\sum_{i=0}^{\infty} A_1^i = (I-A_1)^{-1}\)</span></li>
</ul>
<p>Hence, the stationary solution is <span class="math display">\[
x_t = \mu + \sum_{i=0}^{\infty} A_1^i u_{t-i},
\qquad
\mu = (I-A_1)^{-1}A_0.
\]</span></p>
<p>Equivalently, the MA(<span class="math inline">\(\infty\)</span>) representation can be written using lag operators as <span class="math display">\[
x_t - \mu = (I-A_1L)^{-1} u_t.
\]</span></p>
<p>Under the stability condition, <span class="math inline">\(\{x_t\}\)</span> is covariance-stationary.</p>
</section>
</div>
</section>
<section id="generalization-to-varp" class="level2">
<h2 class="anchored" data-anchor-id="generalization-to-varp">Generalization to VAR(<span class="math inline">\(p\)</span>)</h2>
<p>Consider the VAR(<span class="math inline">\(p\)</span>): <span class="math display">\[
x_t = A_1 x_{t-1} + \cdots + A_p x_{t-p} + u_t.
\]</span></p>
<p>Define the stacked state vector <span class="math display">\[
X_t =
\begin{bmatrix}
x_t \\ x_{t-1} \\ \vdots \\ x_{t-p+1}
\end{bmatrix},
\]</span> so the system can be written in <strong>companion form</strong> <span class="math display">\[
X_t = \mathscr{A} X_{t-1} + \mathscr{U}_t,
\]</span> where <span class="math display">\[
\mathscr{A}
=
\begin{bmatrix}
A_1 &amp; A_2 &amp; \cdots &amp; A_p \\
I   &amp; 0   &amp; \cdots &amp; 0   \\
    &amp; \ddots &amp; \ddots &amp; \vdots \\
0   &amp;       &amp; I &amp; 0
\end{bmatrix}.
\]</span></p>
<p><br></p>
<div class="fragment">
<p>The VAR(<span class="math inline">\(p\)</span>) is <strong>stable</strong> if all eigenvalues of the companion matrix <span class="math inline">\(\mathscr{A}\)</span> lie strictly inside the unit circle:</p>
<p><span class="math display">\[
\det\!\big(I - A_1 z - \cdots - A_p z^p\big) \neq 0
\quad \text{for } |z| \le 1.
\]</span></p>
</div>
</section>
<section id="forecasting-in-var-models" class="level2">
<h2 class="anchored" data-anchor-id="forecasting-in-var-models">Forecasting in VAR Models</h2>
<p>Under stability, the VAR(<span class="math inline">\(p\)</span>) admits the MA(<span class="math inline">\(\infty\)</span>) representation <span class="math display">\[
x_t = \mu + \sum_{j=0}^{\infty} \Psi_j u_{t-j},
\]</span> and the process is causal.</p>
<p>Because future shocks have zero conditional mean, the <span class="math inline">\(h\)</span>-step-ahead forecast based on information at time <span class="math inline">\(t\)</span> is <span class="math display">\[
\widehat{x}_{t+h|t} \equiv \mathbb{E}_t[x_{t+h}]
=
\mu + \sum_{j=h}^{\infty} \Psi_j u_{t+h-j}.
\]</span></p>
<p><br></p>
<div class="fragment">
<p>In practice, forecasts are computed <strong>recursively from the VAR</strong>.</p>
<p>The one-step-ahead forecast is <span class="math display">\[
\widehat{x}_{t+1|t}
=
A_1 x_t + \cdots + A_p x_{t-p+1}.
\]</span></p>
<p>Multi-step forecasts satisfy <span class="math display">\[
\widehat{x}_{t+h|t}
=
A_1 \widehat{x}_{t+h-1|t}
+ \cdots
+ A_p \widehat{x}_{t+h-p|t},
\qquad h \ge 1.
\]</span></p>
</div>
</section>
<section id="forecast-error-variance" class="level2">
<h2 class="anchored" data-anchor-id="forecast-error-variance">Forecast Error Variance</h2>
<p>From the MA(<span class="math inline">\(\infty\)</span>) representation, <span class="math display">\[
x_t = \mu + \sum_{j=0}^{\infty} \Psi_j u_{t-j},
\]</span> the <span class="math inline">\(h\)</span>-step-ahead forecast error is <span class="math display">\[
x_{t+h} - \widehat{x}_{t+h|t}
=
\sum_{j=0}^{h-1} \Psi_j u_{t+h-j}.
\]</span></p>
<p>Let <span class="math inline">\(\Sigma_u = \mathrm{Var}(u_t)\)</span>. The <span class="math inline">\(h\)</span>-step-ahead forecast error variance is <span class="math display">\[
\mathrm{Var}_t\!\big(x_{t+h}-\widehat{x}_{t+h|t}\big)
=
\sum_{j=0}^{h-1} \Psi_j \Sigma_u \Psi_j'.
\]</span></p>
<p>Under stability, this variance increases with <span class="math inline">\(h\)</span> but converges to the unconditional variance of <span class="math inline">\(x_t\)</span>.</p>
<p><br></p>
<div class="fragment">
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">confidence intervals</h3>
<p>Under Gaussian (or asymptotic normality) assumptions, <span class="math display">\[
x_{t+h} \mid \mathscr{F}_t
\;\sim \;
N\!\left(
\widehat{x}_{t+h|t},
\;\mathrm{Var}_t(x_{t+h})
\right).
\]</span> <strong>conditional on the estimated VAR parameters</strong>.</p>
<p>This distribution accounts for uncertainty from future shocks, but <strong>ignores parameter estimation uncertainty</strong>.</p>
<p>For component <span class="math inline">\(i\)</span> of <span class="math inline">\(x_{t+h}\)</span>, a <span class="math inline">\(100(1-\alpha)\%\)</span> forecast confidence interval is <span class="math display">\[
\widehat{x}_{i,t+h|t}
\;\pm\;
z_{1-\alpha/2}
\sqrt{
\left[\mathrm{Var}_t(x_{t+h})\right]_{ii}
}.
\]</span></p>
</section>
</div>
</section>
<section id="computing-forecast-error-variance-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="computing-forecast-error-variance-in-practice">Computing Forecast Error Variance in Practice</h2>
<section id="var1-recursion-you-can-compute-directly" class="level3">
<h3 class="anchored" data-anchor-id="var1-recursion-you-can-compute-directly">VAR(1): recursion you can compute directly</h3>
<p>Suppose <span class="math display">\[
x_t = A_0 + A x_{t-1} + u_t,
\qquad
\mathrm{Var}(u_t)=\Sigma_u.
\]</span></p>
<p>Let the <span class="math inline">\(h\)</span>-step forecast error be <span class="math display">\[
e_{t+h|t} \equiv x_{t+h} - \widehat x_{t+h|t}.
\]</span></p>
<p>Then <span class="math display">\[
e_{t+1|t}=u_{t+1},
\qquad
e_{t+h|t}=A\,e_{t+h-1|t}+u_{t+h}\quad (h\ge2).
\]</span></p>
<p>Taking conditional variances gives the recursion <span class="math display">\[
\Omega_1 = \Sigma_u,
\qquad
\Omega_h = A\,\Omega_{h-1}A' + \Sigma_u,
\qquad
\Omega_h \equiv \mathrm{Var}_t(e_{t+h|t}).
\]</span></p>
<p><br></p>
</section>
<div class="fragment">
<section id="varp-same-idea-via-the-companion-form" class="level3">
<h3 class="anchored" data-anchor-id="varp-same-idea-via-the-companion-form">VAR(<span class="math inline">\(p\)</span>): same idea via the companion form</h3>
<p>Write the VAR(<span class="math inline">\(p\)</span>) in companion form <span class="math display">\[
X_t = \mathscr{A}X_{t-1}+\mathscr{U}_t,
\qquad
\mathscr{U}_t=
\begin{bmatrix}
u_t\\ 0\\ \vdots\\ 0
\end{bmatrix},
\qquad
\mathrm{Var}(\mathscr{U}_t)=
\mathscr{\Sigma}_{\mathscr{U}}=
\begin{bmatrix}
\Sigma_u &amp; 0\\
0 &amp; 0
\end{bmatrix}.
\]</span></p>
<p>Let <span class="math inline">\(\Omega_h^X \equiv \mathrm{Var}_t(X_{t+h}-\widehat X_{t+h|t})\)</span>. Then <span class="math display">\[
\Omega_1^X=\mathscr{\Sigma}_{\mathscr{U}},
\qquad
\Omega_h^X=\mathscr{A}\,\Omega_{h-1}^X\,\mathscr{A}'+\mathscr{\Sigma}_{\mathscr{U}}.
\]</span></p>
<p>The forecast error variance for <span class="math inline">\(x_{t+h}\)</span> (the first <span class="math inline">\(K\)</span> components of <span class="math inline">\(X_{t+h}\)</span>) is <span class="math display">\[
\Omega_h
=
J\,\Omega_h^X\,J',
\qquad
J=\begin{bmatrix} I_K &amp; 0 &amp; \cdots &amp; 0 \end{bmatrix}.
\]</span></p>
</section>
</div>
</section>
<section id="example-forecasting-in-a-stable-var-simulated-data" class="level2">
<h2 class="anchored" data-anchor-id="example-forecasting-in-a-stable-var-simulated-data">Example: Forecasting in a Stable VAR (Simulated Data)</h2>
<p>We simulate a stable bivariate VAR(1) process and illustrate multi-step forecasts and forecast confidence intervals.</p>
<ul>
<li>Data-generating process is stationary by construction</li>
<li>Forecasts are produced recursively from the VAR</li>
<li>Confidence intervals reflect uncertainty from future shocks</li>
</ul>
<div id="18293da8" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="unit7_files/figure-html/cell-2-output-1.png" width="420" height="420" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="dimensionality-in-var-models" class="level2">
<h2 class="anchored" data-anchor-id="dimensionality-in-var-models">Dimensionality in VAR Models</h2>
<p>Consider the reduced-form VAR(<span class="math inline">\(p\)</span>): <span class="math display">\[
x_t = A_1 x_{t-1} + \cdots + A_p x_{t-p} + u_t,
\qquad
\mathrm{Var}(u_t)=\Sigma_u.
\]</span></p>
<p>Let <span class="math inline">\(K\)</span> be the number of variables.</p>
<p>A VAR(<span class="math inline">\(p\)</span>) contains <span class="math display">\[
K^2 p + K
\]</span> parameters to estimate.</p>
<p><br></p>
<div class="fragment">
<section id="consequences" class="level3">
<h3 class="anchored" data-anchor-id="consequences">Consequences</h3>
<ul>
<li>parameter dimension grows <strong>quadratically</strong> in <span class="math inline">\(K\)</span>: <strong>curse of dimensionality</strong></li>
<li>increasing lag length rapidly exhausts degrees of freedom</li>
<li>finite-sample estimation error can be substantial</li>
</ul>
</section>
</div>
<p><br></p>
<div class="fragment">
<section id="rule-of-thumb" class="level3">
<h3 class="anchored" data-anchor-id="rule-of-thumb">Rule of Thumb</h3>
<p>Each equation in a VAR(<span class="math inline">\(p\)</span>) involves approximately <span class="math inline">\(Kp\)</span> slope coefficients.</p>
<p>Empirical practice typically requires <span class="math display">\[
T \gtrsim 5\text{–}10 \times Kp
\]</span> for reliable estimation.</p>
<p>As a result, standard VARs are usually restricted to small systems in macroeconomic applications.</p>
</section>
</div>
</section>
<section id="estimation-and-inference-of-var-models" class="level2">
<h2 class="anchored" data-anchor-id="estimation-and-inference-of-var-models">Estimation and Inference of VAR Models</h2>
<p>A VAR(<span class="math inline">\(p\)</span>) is estimated as a system of linear projections: <span class="math display">\[
x_t = A_1 x_{t-1} + \cdots + A_p x_{t-p} + u_t.
\]</span></p>
<p>Estimation is typically carried out by equation-by-equation ordinary least squares.</p>
<p><br></p>
<div class="fragment">
<section id="inference" class="level3">
<h3 class="anchored" data-anchor-id="inference">Inference</h3>
<p>If:</p>
<ul>
<li>the VAR is stable, so <span class="math inline">\(\{x_t\}\)</span> is stationary</li>
<li>the reduced-form specification is correct (innovations are orthogonal to lagged regressors)</li>
<li>the dimension <span class="math inline">\((K,p)\)</span> is fixed relative to sample size <span class="math inline">\(T\)</span></li>
</ul>
<p>then:</p>
<ul>
<li>OLS estimators are consistent</li>
<li>standard asymptotic theory applies</li>
</ul>
</section>
</div>
<!-- ## VAR as a System of Regressions

Consider the reduced-form VAR($p$):
$$
x_t = A_1 x_{t-1} + \cdots + A_p x_{t-p} + u_t,
\qquad x_t \in \mathbb{R}^K.
$$

The $i$-th equation is
$$
x_{it}
=
Z_t' \beta_i + u_{it},
\qquad
Z_t = (x_{t-1}',\dots,x_{t-p}')'.
$$

Stacking over $t=1,\dots,T$:
$$
x_i = Z \beta_i + u_i,
\qquad i=1,\dots,K,
$$
where **the regressor matrix $Z$ is identical across equations**.

## Stacked System Representation

Stack the $K$ equations across equations:
$$
y \equiv 
\begin{bmatrix}
x_1\\ \vdots\\ x_K
\end{bmatrix}
\in \mathbb{R}^{KT},
\qquad
\beta \equiv
\begin{bmatrix}
\beta_1\\ \vdots\\ \beta_K
\end{bmatrix},
\qquad
u \equiv
\begin{bmatrix}
u_1\\ \vdots\\ u_K
\end{bmatrix}.
$$

Because each equation uses the same regressor matrix $Z$,
the stacked regressor matrix is
$$
X = I_K \otimes Z.
$$

The system can be written compactly as
$$
y = X\beta + u.
$$

<br>

**This is a seemingly unrelated regressions (SUR) system
with identical regressors across equations.**


## Error Covariance Structure

Let
$$
u_t = (u_{1t},\dots,u_{Kt})',
\qquad
\mathrm{Var}(u_t)=\Sigma_u,
$$
allowing contemporaneous correlation across equations.

Assume no serial correlation:
$$
\mathrm{Cov}(u_t,u_s)=0
\quad (t\neq s).
$$

Then the stacked error vector satisfies
$$
\mathrm{Var}(u)=\Omega=\Sigma_u \otimes I_T.
$$


## GLS Estimation of the SUR System

For the system
$$
y = X\beta + u,
\qquad
\mathrm{Var}(u)=\Omega,
$$
the GLS estimator is
$$
\widehat\beta_{\mathrm{GLS}}
=
(X'\Omega^{-1}X)^{-1}X'\Omega^{-1}y,
\qquad
\Omega=\Sigma_u\otimes I_T.
$$

<br>

Using $X = I_K \otimes Z$ and $\Omega^{-1}=\Sigma_u^{-1}\otimes I_T$,
$$
X'\Omega^{-1}X
=
(I_K\otimes Z')(\Sigma_u^{-1}\otimes I_T)(I_K\otimes Z)
=
\Sigma_u^{-1}\otimes (Z'Z).
$$

Hence,
$$
(X'\Omega^{-1}X)^{-1}
=
\Sigma_u \otimes (Z'Z)^{-1}.
$$

Therefore,
$$
\widehat\beta_{\mathrm{GLS}}
=
(I_K \otimes (Z'Z)^{-1}Z')\,y.
$$

But equation-by-equation OLS gives
$$
\widehat\beta_{i,\mathrm{OLS}}=(Z'Z)^{-1}Z'x_i,
\qquad i=1,\dots,K.
$$

Thus,
$$
\widehat\beta_{\mathrm{GLS}}=\widehat\beta_{\mathrm{OLS}}.
$$

Equation-by-equation OLS is efficient for reduced-form VARs. -->
</section>
<section id="what-if-the-variables-are-nonstationary" class="level2">
<h2 class="anchored" data-anchor-id="what-if-the-variables-are-nonstationary">What If the Variables Are Nonstationary?</h2>
<p>In macroeconomic applications, many variables exhibit <strong>stochastic trends</strong>:</p>
<ul>
<li>output, consumption, investment</li>
<li>prices, money, credit aggregates</li>
</ul>
<p><br></p>
<p>A natural response is to difference the data: <span class="math display">\[
\Delta x_t = x_t - x_{t-1}.
\]</span></p>
<p>This guarantees stationarity and allows a VAR to be estimated in differences.</p>
<p><br></p>
<div class="fragment">
<p>However, differencing is not innocuous.</p>
<ul>
<li><p>If variables share common stochastic trends, differencing removes <strong>long-run relationships</strong>.</p></li>
<li><p>As a result, a VAR in differences cannot capture equilibrium comovement in the data.</p></li>
</ul>
<p>(Sims, Stock, and Watson, 1990)</p>
</div>
</section>
<section id="cointegration-and-the-vecm" class="level2">
<h2 class="anchored" data-anchor-id="cointegration-and-the-vecm">Cointegration and the VECM</h2>
<p>Start from a VAR(<span class="math inline">\(p\)</span>) in levels: <span class="math display">\[
x_t = A_1 x_{t-1} + \cdots + A_p x_{t-p} + u_t.
\]</span></p>
<p>Subtract <span class="math inline">\(x_{t-1}\)</span> from both sides: <span class="math display">\[
\Delta x_t
=
\Big(A_1 + \cdots + A_p - I\Big)x_{t-1}
+
\sum_{i=1}^{p-1}
\Big(-A_{i+1}-\cdots-A_p\Big)\Delta x_{t-i}
+
u_t.
\]</span></p>
<p>Define <span class="math display">\[
\Pi := \sum_{i=1}^p A_i - I,
\qquad
\Gamma_i := -\sum_{j=i+1}^p A_j.
\]</span></p>
<p>Then the VAR can always be written as <span class="math display">\[
\Delta x_t
=
\Pi x_{t-1}
+
\sum_{i=1}^{p-1} \Gamma_i \Delta x_{t-i}
+
u_t.
\]</span></p>
<hr>
<p>Suppose <span class="math inline">\(x_t\)</span> is <span class="math inline">\(I(1)\)</span> and <strong>cointegrated</strong> with rank <span class="math inline">\(r&lt;K\)</span>. That is, there exists <span class="math inline">\(\beta \in \mathbb{R}^{K\times r}\)</span> such that <span class="math display">\[
\beta' x_t \ \text{is stationary}.
\]</span> <br></p>
<p>In the VECM form, <span class="math display">\[
\Delta x_t
=
\Pi x_{t-1}
+
\sum_{i=1}^{p-1} \Gamma_i \Delta x_{t-i}
+
u_t,
\]</span> the left-hand side is stationary, as are <span class="math inline">\(\Delta x_{t-i}\)</span> and <span class="math inline">\(u_t\)</span>. Hence <span class="math inline">\(\Pi x_{t-1}\)</span> must be stationary.</p>
<p><br></p>
<p>Since <span class="math inline">\(x_{t-1}\)</span> is nonstationary, <span class="math inline">\(\Pi x_{t-1}\)</span> can be stationary only if it is a <strong>linear combination</strong> of <span class="math inline">\(\beta' x_{t-1}\)</span>.</p>
<p>Equivalently, <span class="math display">\[
\mathrm{rank}(\Pi)=r&lt;K
\quad\Rightarrow\quad
\Pi=\alpha\beta'.
\]</span></p>
<p><br></p>
<div class="fragment">
<p>This yields the <strong>vector error-correction model (VECM)</strong>: <span class="math display">\[
\Delta x_t
=
\alpha \beta' x_{t-1}
+
\sum_{i=1}^{p-1} \Gamma_i \Delta x_{t-i}
+
u_t.
\]</span></p>
<ul>
<li>short-run dynamics: <span class="math inline">\(\Gamma_i \Delta x_{t-i}\)</span></li>
<li>long-run equilibrium errors: <span class="math inline">\(\beta' x_{t-1}\)</span></li>
<li>adjustment back to equilibrium: <span class="math inline">\(\alpha \in \mathbb{R}^{K \times r}\)</span></li>
</ul>
</div>
</section>
<section id="bivariate-example" class="level2">
<h2 class="anchored" data-anchor-id="bivariate-example">Bivariate Example</h2>
<p>Let <span class="math display">\[
x_t =
\begin{bmatrix}
y_t \\ z_t
\end{bmatrix},
\qquad
y_t,\; z_t \text{ are } I(1).
\]</span></p>
<p>Suppose there exists a scalar <span class="math inline">\(\theta\)</span> such that <span class="math display">\[
\beta' x_t = y_t - \theta z_t
\quad \text{is stationary},
\qquad
\beta'=(1,\,-\theta).
\]</span></p>
<p>Then <span class="math inline">\(y_t\)</span> and <span class="math inline">\(z_t\)</span> are <strong>cointegrated</strong> (rank <span class="math inline">\(r=1\)</span>).</p>
<p><br></p>
<div class="fragment">
<p>Define the equilibrium error <span class="math display">\[
e_{t-1} \equiv y_{t-1} - \theta z_{t-1}.
\]</span></p>
<p>The VECM <span class="math display">\[
\Delta x_t
=
\alpha e_{t-1}
+
\sum_{i=1}^{p-1}\Gamma_i \Delta x_{t-i}
+
u_t,
\qquad
\alpha \in \mathbb{R}^{2\times 1},
\]</span> <strong>implies two error-correction equations</strong>: <span class="math display">\[
\begin{aligned}
\Delta y_t &amp;= \alpha_1 e_{t-1} + \text{short-run dynamics} + u_{1t}, \\
\Delta z_t &amp;= \alpha_2 e_{t-1} + \text{short-run dynamics} + u_{2t}.
\end{aligned}
\]</span></p>
</div>
</section>
<section id="estimation-of-the-vecm-parameters" class="level2">
<h2 class="anchored" data-anchor-id="estimation-of-the-vecm-parameters">Estimation of the VECM Parameters</h2>
<p>Consider the VECM <span class="math display">\[
\Delta x_t
=
\Pi x_{t-1}
+
\sum_{i=1}^{p-1}\Gamma_i \Delta x_{t-i}
+
u_t,
\qquad
\Pi=\alpha\beta',
\qquad
\mathrm{rank}(\Pi)=r.
\]</span></p>
<p><br></p>
<div class="fragment">
<section id="what-needs-to-be-estimated" class="level3">
<h3 class="anchored" data-anchor-id="what-needs-to-be-estimated">What Needs to Be Estimated</h3>
<ul>
<li>the short-run parameters <span class="math inline">\(\Gamma_i\)</span></li>
<li>the long-run matrix <span class="math inline">\(\Pi\)</span>, subject to a <strong>rank restriction</strong> <span class="math inline">\(\mathrm{rank}(\Pi)=r&lt;K\)</span></li>
</ul>
</section>
</div>
<p><br></p>
<div class="fragment">
<section id="implication-for-estimation" class="level3">
<h3 class="anchored" data-anchor-id="implication-for-estimation">Implication for Estimation</h3>
<ul>
<li>ordinary least squares treats <span class="math inline">\(\Pi\)</span> as unrestricted</li>
<li>unrestricted OLS produces a full-rank estimate of <span class="math inline">\(\Pi\)</span></li>
</ul>
<p>Therefore, the VECM <strong>cannot be estimated by OLS</strong>.</p>
</section>
</div>
</section>
<section id="johansens-approach-partial-out" class="level2">
<h2 class="anchored" data-anchor-id="johansens-approach-partial-out">Johansen’s Approach (Partial Out)</h2>
<p>Start from the VECM: <span class="math display">\[
\Delta x_t=\Pi x_{t-1}+\sum_{i=1}^{p-1}\Gamma_i \Delta x_{t-i}+u_t.
\]</span></p>
<p><strong>Idea:</strong> partial out short‑run dynamics so the remaining relation is purely long‑run.</p>
<p>Regress <span class="math inline">\(\Delta x_t\)</span> and <span class="math inline">\(x_{t-1}\)</span> on <span class="math inline">\(\{\Delta x_{t-1},\dots,\Delta x_{t-p+1}\}\)</span> and keep residuals: <span class="math display">\[
\begin{aligned}
R_{0t} &amp;= \Delta x_t - \mathbb{E}[\Delta x_t \mid \Delta x_{t-1},\dots,\Delta x_{t-p+1}],\\
R_{1t} &amp;= x_{t-1} - \mathbb{E}[x_{t-1} \mid \Delta x_{t-1},\dots,\Delta x_{t-p+1}].
\end{aligned}
\]</span></p>
<p>Then the model becomes the <strong>long‑run regression</strong> <span class="math display">\[
R_{0t}=\Pi R_{1t}+u_t.
\]</span></p>
<p><br></p>
<div class="fragment">
<p>This follows from the <strong>Frisch-Waugh-Lovell Theorem</strong>. After partialling out, we isolate the cointegrating relationship.</p>
</div>
</section>
<section id="johansens-approach-from-longrun-regression-to-eigenvalues" class="level2">
<h2 class="anchored" data-anchor-id="johansens-approach-from-longrun-regression-to-eigenvalues">Johansen’s Approach (From Long‑Run Regression to Eigenvalues)</h2>
<p>Start from the reduced‑rank least‑squares objective: <span class="math display">\[
\min_{\mathrm{rank}(\Pi)=r}
\frac{1}{T}\sum_{t=1}^T
(R_{0t}-\Pi R_{1t})'(R_{0t}-\Pi R_{1t}).
\]</span></p>
<p>Expand the quadratic form: <span class="math display">\[
\frac{1}{T}\sum_{t=1}^T
\Big(
R_{0t}'R_{0t}
- R_{0t}'\Pi R_{1t}
- R_{1t}'\Pi' R_{0t}
+ R_{1t}'\Pi'\Pi R_{1t}
\Big).
\]</span></p>
<p>Use trace identities: <span class="math inline">\(\mathrm{tr}(x'Ay)=\mathrm{tr}(A y x')\)</span>, <span class="math inline">\(\mathrm{tr}(X)=\mathrm{tr}(X')\)</span>, and <span class="math inline">\(S_{ij}=\frac{1}{T}\sum_{t=1}^T R_{it}R_{jt}'\)</span>: <span class="math display">\[
\frac{1}{T}\sum R_{0t}'R_{0t}=\mathrm{tr}(S_{00}),\qquad
\frac{1}{T}\sum R_{0t}'\Pi R_{1t}=\mathrm{tr}(\Pi S_{10}),
\]</span> <span class="math display">\[
\frac{1}{T}\sum R_{1t}'\Pi'R_{0t}=\mathrm{tr}(S_{01}\Pi'),\qquad
\frac{1}{T}\sum R_{1t}'\Pi'\Pi R_{1t}=\mathrm{tr}(\Pi S_{11}\Pi').
\]</span></p>
<p><br></p>
<div class="fragment">
<p>The objective is equivalent to <span class="math display">\[
\min_{\mathrm{rank}(\Pi)=r}
\mathrm{tr}\!\left(
S_{00}-\Pi S_{10}-S_{01}\Pi' + \Pi S_{11}\Pi'
\right).
\]</span></p>
</div>
<!-- ::: {.fragment}
Differentiate the trace objective w.r.t. $\Pi$ (ignoring the rank constraint first):
$$
J(\Pi)=\mathrm{tr}\!\left(
S_{00}-\Pi S_{10}-S_{01}\Pi' + \Pi S_{11}\Pi'
\right).
$$

Using $\frac{\partial}{\partial \Pi}\mathrm{tr}(\Pi A)=A'$ and
$\frac{\partial}{\partial \Pi}\mathrm{tr}(\Pi B \Pi')=2\Pi B$ (with $B=S_{11}$),
the first‑order condition is
$$
- S_{01} + \Pi S_{11} = 0
\quad\Rightarrow\quad
\Pi = S_{01} S_{11}^{-1}.
$$

Imposing $\mathrm{rank}(\Pi)=r$ leads to the canonical‑correlation solution,
which yields the generalized eigenvalue equation
$$
S_{10}S_{00}^{-1}S_{01}v=\lambda S_{11}v.
$$
::: -->
<hr>
<section id="derivation-why-the-eigenvalue-problem-appears" class="level3">
<h3 class="anchored" data-anchor-id="derivation-why-the-eigenvalue-problem-appears">Derivation (why the eigenvalue problem appears)</h3>
<p>We start from the trace objective <span class="math display">\[
J(\Pi)=\mathrm{tr}\!\left(
S_{00}-\Pi S_{10}-S_{01}\Pi'+\Pi S_{11}\Pi'
\right),
\quad S_{ij}=\frac{1}{T}\sum_{t=1}^T R_{it}R_{jt}'.
\]</span></p>
<p>Impose the rank restriction <span class="math inline">\(\Pi=\alpha\beta'\)</span> with <span class="math inline">\(\alpha,\beta\in\mathbb{R}^{K\times r}\)</span>. Substitute:</p>
<p><span class="math display">\[
J(\alpha,\beta)
=
\mathrm{tr}(S_{00})
-\mathrm{tr}(\alpha\beta' S_{10})
-\mathrm{tr}(S_{01}\beta\alpha')
+\mathrm{tr}(\alpha\beta' S_{11}\beta\alpha').
\]</span></p>
<p><br></p>
<p><strong>Rules (matrix differentials):</strong></p>
<ul>
<li><span class="math inline">\(\mathrm{d}\,\mathrm{tr}(A X)=\mathrm{tr}(A\,\mathrm{d}X)\)</span></li>
<li><span class="math inline">\(\mathrm{d}\,\mathrm{tr}(X A X')=\mathrm{tr}\!\big((XA'+XA)\,\mathrm{d}X'\big)\)</span>. If <span class="math inline">\(A\)</span> is symmetric, this is <span class="math inline">\(2\,\mathrm{tr}(X A\,\mathrm{d}X')\)</span>. <!-- - $\mathrm{tr}(X)=\mathrm{tr}(X')$ and cyclicity: $\mathrm{tr}(ABC)=\mathrm{tr}(BCA)$. --></li>
</ul>
<hr>
</section>
<section id="step-1-differentiate-w.r.t.-alpha-holding-beta-fixed" class="level3">
<h3 class="anchored" data-anchor-id="step-1-differentiate-w.r.t.-alpha-holding-beta-fixed">Step 1: Differentiate w.r.t. <span class="math inline">\(\alpha\)</span> (holding <span class="math inline">\(\beta\)</span> fixed)</h3>
<p>Use cyclicity to group <span class="math inline">\(\alpha\)</span>: <span class="math display">\[
J(\alpha,\beta)=
\mathrm{tr}(S_{00})
-2\,\mathrm{tr}(\alpha\beta' S_{10})
+\mathrm{tr}\!\big(\alpha(\beta' S_{11}\beta)\alpha'\big).
\]</span></p>
<p>Let <span class="math inline">\(B=\beta' S_{11}\beta\)</span> (this is <span class="math inline">\(r\times r\)</span> and symmetric). Then <span class="math display">\[
\mathrm{d}J
=
-2\,\mathrm{tr}(\beta' S_{10}\,\mathrm{d}\alpha)
+2\,\mathrm{tr}(\alpha B\,\mathrm{d}\alpha').
\]</span></p>
<p>Convert the second term using <span class="math inline">\(\mathrm{tr}(M\,\mathrm{d}\alpha')=\mathrm{tr}(M'\,\mathrm{d}\alpha)\)</span>: <span class="math display">\[
\mathrm{d}J
=
-2\,\mathrm{tr}(\beta' S_{10}\,\mathrm{d}\alpha)
+2\,\mathrm{tr}((\alpha B)'\,\mathrm{d}\alpha).
\]</span></p>
<p>Set <span class="math inline">\(\mathrm{d}J=0\)</span> for all <span class="math inline">\(\mathrm{d}\alpha\)</span>: <span class="math display">\[
-(\beta' S_{10}) + (\alpha B)'=0
\quad\Rightarrow\quad
\alpha B = S_{01}\beta.
\]</span></p>
<p>So <span class="math display">\[
\widehat{\alpha}=S_{01}\beta(\beta' S_{11}\beta)^{-1}.
\]</span></p>
<hr>
</section>
<section id="step-2-plug-back-to-get-a-problem-in-beta-only" class="level3">
<h3 class="anchored" data-anchor-id="step-2-plug-back-to-get-a-problem-in-beta-only">Step 2: Plug back to get a problem in <span class="math inline">\(\beta\)</span> only</h3>
<p>Substitute <span class="math inline">\(\widehat{\alpha}\)</span> into <span class="math inline">\(J(\alpha,\beta)\)</span> and simplify: <span class="math display">\[
J(\beta)
=
\mathrm{tr}(S_{00})
-
\mathrm{tr}\!\Big[
(\beta' S_{10} S_{00}^{-1} S_{01}\beta)
(\beta' S_{11}\beta)^{-1}
\Big].
\]</span></p>
<p>Thus maximizing the fit is equivalent to <span class="math display">\[
\max_{\beta}\;
\mathrm{tr}\!\Big[
(\beta' S_{10} S_{00}^{-1} S_{01}\beta)
(\beta' S_{11}\beta)^{-1}
\Big].
\]</span></p>
<p><br></p>
<div class="fragment">
<p>Let <span class="math inline">\(A=S_{10}S_{00}^{-1}S_{01}\)</span>.<br>
Normalize <span class="math inline">\(\beta'S_{11}\beta=I_r\)</span> and solve <span class="math display">\[
\max_{\beta}\ \mathrm{tr}(\beta' A \beta)
\quad \text{s.t.}\quad \beta' S_{11} \beta = I_r.
\]</span></p>
<p>Lagrangian: <span class="math display">\[
\mathscr{L}(\beta,\Lambda)
=
\mathrm{tr}(\beta' A \beta)
-
\mathrm{tr}\!\big[\Lambda(\beta' S_{11} \beta - I_r)\big].
\]</span></p>
<p>FOC w.r.t. <span class="math inline">\(\beta\)</span>: <span class="math display">\[
2A\beta-2S_{11}\beta\Lambda=0
\quad\Rightarrow\quad
A\beta=S_{11}\beta\Lambda.
\]</span></p>
<!-- (\Lambda) is symmetric because of the symmetry in the constraint \beta' B \beta
It can also be diagonal WLOG. \beta is not unique. Let Q be an orthogonal matrix,\betaQ is also a solution.  A \beta Q = \beta Q (Q' \Lambda Q), and we can choose Q such that Q' \Lambda Q is diagonal, and take it as our Lambda. This is basically a normalization.
Alternatively, because the objective and constraint are invariant to an (r\times r) rotation of (\beta), we can choose a basis in the (\beta)-space that diagonalizes (\Lambda). 
-->
<p>Thus each column <span class="math inline">\(v\)</span> of <span class="math inline">\(\beta\)</span> satisfies <span class="math display">\[
A v=\lambda S_{11} v,
\quad\text{i.e.}\quad
S_{10}S_{00}^{-1}S_{01}v=\lambda S_{11}v.
\]</span></p>
</div>
<hr>
</section>
<section id="step-3-eigenvalue-characterization" class="level3">
<h3 class="anchored" data-anchor-id="step-3-eigenvalue-characterization">Step 3: Eigenvalue characterization</h3>
<p>This is a canonical‑correlation problem. The FOCs imply: <span class="math display">\[
S_{10}S_{00}^{-1}S_{01}v=\lambda S_{11}v.
\]</span></p>
<p><br></p>
<p>Since <span class="math inline">\(S_{11}\)</span> is invertible, multiply by <span class="math inline">\(S_{11}^{-1}\)</span>: <span class="math display">\[
S_{11}^{-1}S_{10}S_{00}^{-1}S_{01}v=\lambda v.
\]</span></p>
<p>So <span class="math inline">\(v\)</span> is an eigenvector of <span class="math inline">\(S_{11}^{-1}S_{10}S_{00}^{-1}S_{01}\)</span>.</p>
<p><br></p>
<p>The top <span class="math inline">\(r\)</span> eigenvectors <span class="math inline">\(v\)</span> give <span class="math inline">\(\widehat{\beta}\)</span>, and <span class="math inline">\(\widehat{\alpha}=S_{01}\widehat{\beta}\)</span>.</p>
</section>
</section>
<section id="johansens-approach-gaussian-mle-setup" class="level2">
<h2 class="anchored" data-anchor-id="johansens-approach-gaussian-mle-setup">Johansen’s Approach (Gaussian MLE Setup)</h2>
<p>Assume the VECM errors are Gaussian: <span class="math display">\[
u_t \sim N(0,\Sigma_u), \quad t=1,\dots,T.
\]</span></p>
<p>After partialling out short‑run dynamics we have <span class="math display">\[
R_{0t}=\Pi R_{1t}+\varepsilon_t,
\]</span> where <span class="math display">\[
\varepsilon_t
=
u_t-\mathbb{E}[u_t\mid \Delta x_{t-1},\dots,\Delta x_{t-p+1}],
\]</span> so <span class="math inline">\(\varepsilon_t\)</span> is the <strong>residualized error</strong>.</p>
<p>Under Gaussianity, <span class="math inline">\(\varepsilon_t \sim N(0,\Sigma_\varepsilon)\)</span>. The (conditional) log‑likelihood is <span class="math display">\[
\ell(\Pi,\Sigma_\varepsilon)
=
-\frac{T}{2}\log|\Sigma_\varepsilon|
-\frac{1}{2}\sum_{t=1}^T
(R_{0t}-\Pi R_{1t})'\Sigma_\varepsilon^{-1}(R_{0t}-\Pi R_{1t})
+ \text{const}.
\]</span></p>
<p><br></p>
<div class="fragment">
<p><strong>Key point:</strong> maximizing the likelihood is equivalent to minimizing the weighted sum of squared residuals.</p>
</div>
</section>
<section id="johansens-approach-profile-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="johansens-approach-profile-likelihood">Johansen’s Approach (Profile Likelihood)</h2>
<p>Let <span class="math inline">\(e_t=R_{0t}-\Pi R_{1t}\)</span> and <span class="math inline">\(E=[e_1,\dots,e_T]\)</span>. Then <span class="math inline">\(\sum_{t=1}^T e_t e_t' = EE'\)</span>.</p>
<p>The Gaussian log‑likelihood is <span class="math display">\[
\ell(\Pi,\Sigma_\varepsilon)
=
-\frac{T}{2}\log|\Sigma_\varepsilon|
-\frac{1}{2}\mathrm{tr}(\Sigma_\varepsilon^{-1}EE')
+\text{const}.
\]</span></p>
<p>For fixed <span class="math inline">\(\Pi\)</span>, maximize over <span class="math inline">\(\Sigma_\varepsilon\)</span>: <span class="math display">\[
\widehat{\Sigma}_\varepsilon(\Pi)=\frac{1}{T}EE'.
\]</span></p>
<p>Plug in: <span class="math display">\[
\ell(\Pi)
=
-\frac{T}{2}\log\!\left|\widehat{\Sigma}_\varepsilon(\Pi)\right|
-\frac{1}{2}\mathrm{tr}\!\left(\widehat{\Sigma}_\varepsilon^{-1}EE'\right)
+ \text{const}.
\]</span></p>
<p>But <span class="math display">\[
\mathrm{tr}\!\left(\widehat{\Sigma}_\varepsilon^{-1}EE'\right)
=
\mathrm{tr}\!\left(\left(\tfrac{1}{T}EE'\right)^{-1}EE'\right)
=
\mathrm{tr}(T I_K)
= TK,
\]</span> which is constant in <span class="math inline">\(\Pi\)</span>.</p>
<p>Therefore <span class="math display">\[
\ell(\Pi)
=
-\frac{T}{2}\log\!\left|\widehat{\Sigma}_\varepsilon(\Pi)\right|
+ \text{const}.
\]</span></p>
<p>Thus, <span class="math display">\[
\max_{\Pi:\,\mathrm{rank}(\Pi)=r}\ \ell(\Pi)
\quad\Longleftrightarrow\quad
\min_{\Pi:\,\mathrm{rank}(\Pi)=r}\ \left|\widehat{\Sigma}_\varepsilon(\Pi)\right|.
\]</span></p>
<hr>
<p>Start from the profile likelihood: <span class="math display">\[
\ell(\Pi)\propto -\frac{T}{2}\log\left|\widehat\Sigma_\varepsilon(\Pi)\right|,
\quad
\widehat\Sigma_\varepsilon(\Pi)
=
S_{00}-\Pi S_{10}-S_{01}\Pi'+\Pi S_{11}\Pi'.
\]</span></p>
<p>Impose <span class="math inline">\(\Pi=\alpha\beta'\)</span> (rank <span class="math inline">\(r\)</span>). Let <span class="math inline">\(C=S_{01}\beta\)</span> and <span class="math inline">\(B=\beta'S_{11}\beta\)</span>. Then <span class="math display">\[
\widehat\Sigma_\varepsilon(\alpha,\beta)
=
S_{00}-\alpha C' - C\alpha' + \alpha B \alpha'.
\]</span></p>
<p>Complete the square: <span class="math display">\[
\widehat\Sigma_\varepsilon(\alpha,\beta)
=
\underbrace{S_{00}-C B^{-1} C'}_{\Sigma_*}
+
(\alpha - C B^{-1})\, B\,(\alpha - C B^{-1})'
=
(I + LL'\Sigma_*^{-1})\Sigma_*,
\]</span> where <span class="math inline">\(L := (\alpha - C B^{-1})\,B^{1/2}\)</span>.</p>
<p>Since determinant is invariant to similarity transformation, and <span class="math display">\[
\Sigma_*^{-1/2}(I + LL'\Sigma_*^{-1})\Sigma_*^{1/2} = I + \Sigma_*^{-1/2}LL'\Sigma_*^{-1/2},
\]</span></p>
<p>we have <span class="math display">\[
|\widehat\Sigma_\varepsilon(\alpha,\beta)|
=
|\Sigma_*|\;\Big|I + \Sigma_*^{-1/2}LL'\Sigma_*^{-1/2}\Big|.
\]</span></p>
<p>Set <span class="math inline">\(Z=\Sigma_*^{-1/2}L\)</span>. Then the second determinant is <span class="math inline">\(|I+ZZ'|\ge 1\)</span>, with equality iff <span class="math inline">\(Z=0\)</span> iff <span class="math inline">\(\alpha=CB^{-1}\)</span>.</p>
<hr>
<p>Plugging in <span class="math inline">\(\widehat\alpha=S_{01}\beta(\beta'S_{11}\beta)^{-1}\)</span> gives <span class="math display">\[
\widehat\Sigma_\varepsilon(\beta)
=
S_{00}-S_{01}\beta(\beta'S_{11}\beta)^{-1}\beta'S_{10}.
\]</span></p>
<p>Using the determinant identity <span class="math inline">\(|A-B|=|A|\;|I-A^{-1}B|\)</span>, <span class="math display">\[
|\widehat\Sigma_\varepsilon(\beta)|
=
|S_{00}|\;
\Big|I - S_{00}^{-1}S_{01}\beta(\beta'S_{11}\beta)^{-1}\beta'S_{10}\Big|.
\]</span></p>
<p>Since <span class="math inline">\(|S_{00}|\)</span> does not depend on <span class="math inline">\(\beta\)</span>, maximizing the likelihood is equivalent to minimizing the second determinant. Using <span class="math inline">\(|I+AB|=|I+BA|\)</span>, <span class="math display">\[
\Big|I - S_{00}^{-1}S_{01}\beta(\beta'S_{11}\beta)^{-1}\beta'S_{10}\Big|
=
\Big|I - (\beta'S_{10}S_{00}^{-1}S_{01}\beta)(\beta'S_{11}\beta)^{-1}\Big|.
\]</span></p>
<p>Because <span class="math inline">\(\beta\)</span> is identified only up to right multiplication by a nonsingular <span class="math inline">\(r\times r\)</span> matrix, we impose the normalization <span class="math display">\[
\beta'S_{11}\beta = I_r.
\]</span></p>
<p>Under this normalization, maximizing the likelihood is equivalent to <span class="math display">\[
\max_{\beta}\;
\Big|\beta' S_{10} S_{00}^{-1} S_{01}\beta\Big|
\quad\text{s.t.}\quad
\beta' S_{11}\beta = I_r.
\]</span></p>
<hr>
<p>Under the normalization <span class="math inline">\(\beta'S_{11}\beta=I_r\)</span>, consider the determinant problem <span class="math display">\[
\max_{\beta}\;|\beta'A\beta|
\quad\text{s.t.}\quad
\beta'S_{11}\beta=I_r,
\qquad
A:=S_{10}S_{00}^{-1}S_{01}.
\]</span> Since <span class="math inline">\(\log(\cdot)\)</span> is strictly increasing on <span class="math inline">\((0,\infty)\)</span>, this is equivalent to <span class="math display">\[
\max_{\beta:\,\beta'S_{11}\beta=I_r}\ \log|\beta'A\beta|.
\]</span></p>
<p>Let <span class="math inline">\(M(\beta):=\beta'A\beta\)</span>. By Jacobi’s formula, <span class="math display">\[
d\log|M|=\mathrm{tr}(M^{-1}dM).
\]</span> Moreover, <span class="math display">\[
dM = d(\beta'A\beta) = (d\beta)'A\beta+\beta'A(d\beta),
\]</span> so (using symmetry of <span class="math inline">\(A\)</span> and cyclicity of the trace) <span class="math display">\[
d\log|M|
=
2\,\mathrm{tr}\!\Big(\big(A\beta M^{-1}\big)'d\beta\Big).
\]</span></p>
<p>With the Lagrangian <span class="math display">\[
\mathscr L(\beta,\Lambda)
=
\log|\beta'A\beta|
-
\mathrm{tr}\!\big(\Lambda(\beta'S_{11}\beta-I_r)\big),
\qquad \Lambda=\Lambda',
\]</span> we have <span class="math display">\[
d\,\mathscr L
=
2\,\mathrm{tr}\!\Big((A\beta M^{-1})'d\beta\Big)
-
2\,\mathrm{tr}\!\Big((S_{11}\beta\Lambda)'d\beta\Big).
\]</span> Since <span class="math inline">\(d\beta\)</span> is arbitrary, the first-order condition is <span class="math display">\[
A\beta(\beta'A\beta)^{-1}=S_{11}\beta\Lambda.
\]</span> Right-multiplying by <span class="math inline">\((\beta'A\beta)\)</span> yields <span class="math display">\[
A\beta = S_{11}\beta\Gamma,
\qquad \Gamma:=\Lambda(\beta'A\beta),
\]</span> and after an orthogonal rotation of the columns of <span class="math inline">\(\beta\)</span> we may take <span class="math inline">\(\Gamma=\mathrm{diag}(\lambda_1,\dots,\lambda_r)\)</span>, giving the generalized eigenvalue system <span class="math display">\[
Av=\lambda S_{11}v,
\qquad\text{i.e.}\qquad
S_{10}S_{00}^{-1}S_{01}v=\lambda S_{11}v.
\]</span></p>
</section>
<section id="johansens-approach-lr-rank-tests" class="level2">
<h2 class="anchored" data-anchor-id="johansens-approach-lr-rank-tests">Johansen’s Approach (LR Rank Tests)</h2>
<p>For each <span class="math inline">\(r\)</span>, compare the maximized likelihood under <span class="math inline">\(H_0:\mathrm{rank}(\Pi)\le r\)</span> to the unrestricted model.</p>
<p><span class="math display">\[
\mathrm{LR}(r)=-2(\ell_r-\ell_K)
=
-T\log\frac{|\widehat\Sigma_r|}{|\widehat\Sigma_K|}.
\]</span></p>
<p><br></p>
<div class="fragment">
<section id="unrestricted-residual-covariance" class="level3">
<h3 class="anchored" data-anchor-id="unrestricted-residual-covariance">Unrestricted Residual Covariance</h3>
<p>From the partialled-out long-run regression</p>
<p><span class="math display">\[
R_{0t} = \Pi R_{1t} + u_t
\]</span></p>
<p>the unrestricted OLS estimator is</p>
<p><span class="math display">\[
\widehat\Pi_{\text{OLS}}=S_{01}S_{11}^{-1}.
\]</span></p>
<p>Plugging into <span class="math display">\[
\widehat\Sigma(\Pi)
=
S_{00}-\Pi S_{10}-S_{01}\Pi'+\Pi S_{11}\Pi',
\]</span> gives</p>
<p><span class="math display">\[
\widehat\Sigma_K =
S_{00}-S_{01}S_{11}^{-1}S_{10}
\]</span></p>
</section>
</div>
<p><br></p>
<div class="fragment">
<section id="restricted-residual-covariance" class="level3">
<h3 class="anchored" data-anchor-id="restricted-residual-covariance">Restricted Residual Covariance</h3>
<p><span class="math display">\[
\widehat\Sigma_r
=
S_{00}
-
S_{01}\beta_r\beta_r'S_{10},
\]</span> where <span class="math inline">\(\beta_r\)</span> consists of the first <span class="math inline">\(r\)</span> eigenvectors solving</p>
<p><span class="math display">\[
S_{11}^{-1}S_{10}S_{00}^{-1}S_{01}v=\lambda v.
\]</span></p>
</section>
</div>
<hr>
<section id="determinant-of-unrestricted-residual-covariance-widehatsigma_k" class="level3">
<h3 class="anchored" data-anchor-id="determinant-of-unrestricted-residual-covariance-widehatsigma_k">Determinant of Unrestricted Residual Covariance: <span class="math inline">\(|\widehat\Sigma_K|\)</span></h3>
<p>Define <span class="math display">\[
C := S_{00}^{-1/2} S_{01} S_{11}^{-1/2}.
\]</span> Then the unrestricted residual covariance can be written as <span class="math display">\[
\widehat\Sigma_K
=
S_{00}^{1/2}\big(I - CC'\big)S_{00}^{1/2},
\]</span> and therefore <span class="math display">\[
|\widehat\Sigma_K|
=
|S_{00}|\;|I-CC'|.
\]</span></p>
<p><br></p>
<div class="fragment">
<p>To evaluate <span class="math inline">\(|I-CC'|\)</span>, note that <span class="math inline">\(CC'\)</span> is symmetric and positive semidefinite, so it admits an orthogonal eigen-decomposition <span class="math display">\[
CC' = Q\Lambda Q',
\qquad Q'Q=I,
\qquad
\Lambda=\mathrm{diag}(\hat\lambda_1,\dots,\hat\lambda_K).
\]</span> Then <span class="math display">\[
I-CC' = I-Q\Lambda Q' = Q(I-\Lambda)Q'.
\]</span> Since similarity preserves eigenvalues and the determinant equals the product of eigenvalues, <span class="math display">\[
|I-CC'|
=
|I-\Lambda|
=
\prod_{i=1}^K (1-\hat\lambda_i).
\]</span></p>
<p>Hence <span class="math display">\[
|\widehat\Sigma_K|
=
|S_{00}|\prod_{i=1}^K (1-\hat\lambda_i).
\]</span></p>
</div>
<hr>
</section>
<section id="determinant-of-restricted-residual-covariance-widehatsigma_r" class="level3">
<h3 class="anchored" data-anchor-id="determinant-of-restricted-residual-covariance-widehatsigma_r">Determinant of Restricted Residual Covariance: <span class="math inline">\(|\widehat\Sigma_r|\)</span></h3>
<p>The restricted residual covariance is <span class="math display">\[
\widehat\Sigma_r
=
S_{00}-S_{01}\beta_r\beta_r'S_{10}.
\]</span> Pre- and post-multiply by <span class="math inline">\(S_{00}^{-1/2}\)</span>: <span class="math display">\[
S_{00}^{-1/2}\widehat\Sigma_r S_{00}^{-1/2}
=
I
-
S_{00}^{-1/2} S_{01}\beta_r\beta_r'S_{10} S_{00}^{-1/2}.
\]</span> Factor the second term on the right-hand side: <span class="math display">\[
S_{00}^{-1/2} S_{01}\beta_r\beta_r'S_{10} S_{00}^{-1/2}
=
\big(S_{00}^{-1/2}S_{01}S_{11}^{-1/2}\big)
\big(S_{11}^{1/2}\beta_r\beta_r'S_{11}^{1/2}\big)
\big(S_{11}^{-1/2}S_{10}S_{00}^{-1/2}\big).
\]</span></p>
<p><br></p>
<div class="fragment">
<p>Using <span class="math inline">\(C=S_{00}^{-1/2}S_{01}S_{11}^{-1/2}\)</span> and <span class="math inline">\(S_{10}=S_{01}'\)</span>, this becomes <span class="math display">\[
S_{00}^{-1/2} S_{01}\beta_r\beta_r'S_{10} S_{00}^{-1/2}
=
C P_r C',
\qquad
P_r := S_{11}^{1/2}\beta_r\beta_r'S_{11}^{1/2}.
\]</span></p>
<p>Let <span class="math inline">\(W_r:=S_{11}^{1/2}\beta_r\)</span>. Since <span class="math inline">\(\beta_r'S_{11}\beta_r=I_r\)</span>, we have <span class="math inline">\(W_r'W_r=I_r\)</span>, so <span class="math display">\[
P_r=W_rW_r'
\]</span> is the orthogonal projector onto <span class="math inline">\(\mathrm{span}(W_r)\)</span>.</p>
</div>
<p><br></p>
<div class="fragment">
<p>Therefore,</p>
<p><span class="math display">\[
|\widehat\Sigma_r| = |S_{00}| |I - CP_r C'|
\]</span></p>
</div>
<hr>
<p>Recall that <span class="math display">\[
C := S_{00}^{-1/2}S_{01}S_{11}^{-1/2}, \quad CC' = Q \Lambda Q',
\quad
\Lambda=\mathrm{diag}(\hat\lambda_1,\dots,\hat\lambda_K),
\ \hat\lambda_1\ge\cdots\ge\hat\lambda_K.
\]</span></p>
<p><br></p>
<p>From the rank-restricted likelihood problem, the cointegration vectors <span class="math inline">\(\beta_r\)</span> satisfy <span class="math display">\[
S_{10}S_{00}^{-1}S_{01}v=\lambda S_{11}v.
\]</span> Equivalently, <span class="math display">\[
\begin{align}
S_{11}^{-1/2} S_{10} S_{00}^{-1} S_{01} S_{11}^{-1/2} w
=&amp;
\lambda w,
\\
(C'C) w=&amp; \lambda w,  \qquad w=S_{11}^{1/2}v.
\end{align}
\]</span></p>
<p>Therefore the generalized eigenvalues from the Johansen problem are the eigenvalues of <span class="math inline">\(C'C\)</span>, which share the <strong>same nonzero eigenvalues</strong> as <span class="math inline">\(CC'\)</span>.</p>
<p><br></p>
<p>Let <span class="math inline">\(W_r:=S_{11}^{1/2}\beta_r\)</span> collects the eigenvectors <span class="math inline">\(w_1,\dots,w_r\)</span> of <span class="math inline">\(C'C\)</span> associated with <span class="math inline">\(\hat\lambda_1,\dots,\hat\lambda_r\)</span>, then <span class="math display">\[
P_r := W_rW_r'
\]</span> is the orthogonal projector onto this eigenspace.</p>
<hr>
</section>
<section id="eigenstructure-of-cp_rc" class="level3">
<h3 class="anchored" data-anchor-id="eigenstructure-of-cp_rc">Eigenstructure of <span class="math inline">\(CP_rC'\)</span></h3>
<p>We evaluate the eigenvalues of <span class="math inline">\(CP_rC'\)</span>.</p>
<p>Let <span class="math inline">\(w_i\)</span> satisfy <span class="math display">\[
C'C w_i=\hat\lambda_i w_i,
\qquad \hat\lambda_i\ge 0.
\]</span> For <span class="math inline">\(\hat\lambda_i&gt;0\)</span>, note that <span class="math display">\[
CC'(Cw_i)=\hat\lambda_i(Cw_i),
\]</span> so <span class="math inline">\(Cw_i\)</span> is an eigenvector of <span class="math inline">\(CC'\)</span> with eigenvalue <span class="math inline">\(\hat\lambda_i\)</span>.</p>
<p><br></p>
<p>Now consider the effect of the projection <span class="math inline">\(P_r\)</span>:</p>
<ul>
<li><p><strong>If <span class="math inline">\(i\le r\)</span></strong>, then <span class="math inline">\(w_i\in\mathrm{span}(W_r)\)</span> and <span class="math inline">\(P_r w_i=w_i\)</span>, hence <span class="math display">\[
CP_rC'(Cw_i)=\hat\lambda_i\,Cw_i.
\]</span></p></li>
<li><p><strong>If <span class="math inline">\(i&gt;r\)</span></strong>, then <span class="math inline">\(w_i\perp\mathrm{span}(W_r)\)</span> and <span class="math inline">\(P_r w_i=0\)</span>, hence <span class="math display">\[
CP_rC'(Cw_i)=0.
\]</span></p></li>
</ul>
<p><br></p>
<div class="fragment">
<p><strong>Conclusion.</strong> The eigenvalues of <span class="math inline">\(CP_rC'\)</span> are <span class="math display">\[
\hat\lambda_1,\dots,\hat\lambda_r,0,\dots,0,
\]</span> and therefore <span class="math display">\[
|I-CP_rC'|
=
\prod_{i=1}^r (1-\hat\lambda_i).
\]</span></p>
</div>
</section>
</section>
<section id="johansens-likelihood-ratio-tests-for-cointegration-rank" class="level2">
<h2 class="anchored" data-anchor-id="johansens-likelihood-ratio-tests-for-cointegration-rank">Johansen’s Likelihood Ratio Tests for Cointegration Rank</h2>
<p>For a given rank restriction <span class="math inline">\(r\)</span>, the likelihood ratio statistic compares</p>
<ul>
<li>the <strong>restricted model</strong> with <span class="math inline">\(\mathrm{rank}(\Pi)=r\)</span></li>
<li>the <strong>unrestricted model</strong> with <span class="math inline">\(\mathrm{rank}(\Pi)=K\)</span></li>
</ul>
<p>The likelihood ratio is <span class="math display">\[
\mathrm{LR}(r)
=
-2(\ell_r-\ell_K)
=
-T\log\frac{|\widehat\Sigma_r|}{|\widehat\Sigma_K|}.
\]</span></p>
<p><br></p>
<p>From the determinant calculations, <span class="math display">\[
|\widehat\Sigma_K|
=
|S_{00}|\prod_{i=1}^K(1-\hat\lambda_i),
\qquad
|\widehat\Sigma_r|
=
|S_{00}|\prod_{i=r+1}^K(1-\hat\lambda_i).
\]</span></p>
<p>Therefore, <span class="math display">\[
\frac{|\widehat\Sigma_r|}{|\widehat\Sigma_K|}
=
\prod_{i=r+1}^K(1-\hat\lambda_i),
\]</span> and <span class="math display">\[
\mathrm{LR}(r)
=
-T\sum_{i=r+1}^K \log(1-\hat\lambda_i).
\]</span></p>
<hr>
<section id="two-lr-tests-what-is-being-tested" class="level3">
<h3 class="anchored" data-anchor-id="two-lr-tests-what-is-being-tested">Two LR Tests: What Is Being Tested?</h3>
<p>Given likelihood ratio, <span class="math display">\[
\frac{|\widehat\Sigma_r|}{|\widehat\Sigma_K|}
=
\prod_{i=r+1}^K (1-\hat\lambda_i),
\]</span></p>
<p><br></p>
<div class="fragment">
<section id="trace-test-joint-lr-test" class="level4">
<h4 class="anchored" data-anchor-id="trace-test-joint-lr-test">Trace Test (Joint LR Test)</h4>
<p>Compares the rank-<span class="math inline">\(r\)</span> model to the unrestricted model: <span class="math display">\[
\mathrm{LR}_{\text{trace}}(r)
=
-2(\ell_r-\ell_K)
=
-T\sum_{i=r+1}^K \log(1-\hat\lambda_i).
\]</span></p>
<p>This tests <span class="math display">\[
H_0:\ \mathrm{rank}(\Pi)\le r
\quad \text{vs.} \quad
H_1:\ \mathrm{rank}(\Pi)&gt;r.
\]</span></p>
<p>All remaining directions are tested <strong>jointly</strong>: the null requires <em>every</em> <span class="math inline">\(\hat\lambda_{r+1},\dots,\hat\lambda_K\)</span> to be zero.</p>
</section>
</div>
<p><br></p>
<div class="fragment">
<section id="maximum-eigenvalue-test-sequential-lr-test" class="level4">
<h4 class="anchored" data-anchor-id="maximum-eigenvalue-test-sequential-lr-test">Maximum Eigenvalue Test (Sequential LR Test)</h4>
<p>The maximum eigenvalue test is used <strong>sequentially</strong>, starting from <span class="math inline">\(r=0\)</span>.</p>
<p>At each step <span class="math inline">\(r\)</span>, compare the rank-<span class="math inline">\(r\)</span> model to the rank-<span class="math inline">\((r+1)\)</span> model: <span class="math display">\[
\mathrm{LR}_{\max}(r,r+1)
=
-2(\ell_r-\ell_{r+1})
=
-T\log(1-\hat\lambda_{r+1}).
\]</span></p>
<p>This tests <span class="math display">\[
H_0:\ \mathrm{rank}(\Pi)=r
\quad \text{vs.} \quad
H_1:\ \mathrm{rank}(\Pi)=r+1.
\]</span></p>
<p>If <span class="math inline">\(H_0\)</span> is rejected, increase <span class="math inline">\(r\)</span> by one and repeat the test. The selected rank is the first <span class="math inline">\(r\)</span> for which the null is <strong>not rejected</strong>.</p>
</section>
</div>
<!-- ## Why the LR Test Is for $\mathrm{rank}(\Pi)\le r$

Although the estimator $\widehat\Pi_r$ is computed under the constraint
$\mathrm{rank}(\Pi)=r$, the likelihood ratio test concerns whether this
constraint is **binding for the true parameter**.

<br>

::: {.fragment}
#### Key Observation (Projection Argument)

Consider the long-run regression
$$
R_{0t} = \Pi R_{1t} + u_t.
$$

Let $\Pi_0=\alpha_0\beta_0'$ be the true parameter with
$\mathrm{rank}(\Pi_0)=r_0<r$.

Then the fitted values satisfy
$$
\Pi_0 R_{1t} \in \mathscr{S}_0 := \mathrm{span}(\beta_0),
\qquad \dim(\mathscr{S}_0)=r_0.
$$

Any rank-$r$ matrix $\Pi$ whose column space contains $\mathscr{S}_0$
produces **identical fitted values**:
$$
\Pi R_{1t} = \Pi_0 R_{1t}.
$$
:::

<br>

::: {.fragment}
#### Consequence for Estimation

The rank-$r$ estimator $\widehat\Pi_r=\widehat\alpha_r\widehat\beta_r'$
projects $R_{0t}$ onto an $r$-dimensional space.

If the true rank satisfies $r_0<r$, then asymptotically:

- the additional $r-r_0$ directions carry **zero canonical correlation**
- the fitted values coincide with the unrestricted fit
- residuals are unchanged:
$$
R_{0t}-\widehat\Pi_r R_{1t}
=
R_{0t}-\widehat\Pi_K R_{1t}.
$$
:::

---

#### Implication for the Likelihood

Since the Gaussian likelihood depends on $\Pi$ only through the residual
covariance,
$$
\widehat\Sigma_\varepsilon(\Pi)
=
\frac{1}{T}\sum_{t=1}^T
(R_{0t}-\Pi R_{1t})(R_{0t}-\Pi R_{1t})',
$$
we have
$$
\widehat\Sigma_r \;\xrightarrow{p}\; \widehat\Sigma_K
\quad\text{if } r_0\le r.
$$

Therefore, the rank-$r$ restriction is **asymptotically slack** whenever
$\mathrm{rank}(\Pi_0)\le r$.


<br>

::: {.fragment}
#### Conclusion

The likelihood ratio test compares the maximized likelihood under
$\mathrm{rank}(\Pi)=r$ to the unrestricted model and therefore tests
$$
H_0:\ \mathrm{rank}(\Pi_0)\le r
\quad \text{vs.} \quad
H_1:\ \mathrm{rank}(\Pi_0)>r.
$$

This explains why the null hypothesis is stated as an inequality,
even though estimation imposes equality.
::: -->
</section>
</section>
<section id="johansens-approach-summary" class="level2">
<h2 class="anchored" data-anchor-id="johansens-approach-summary">Johansen’s Approach — Summary</h2>
<ul>
<li>Treat the VECM as a <strong>system</strong>, estimating long-run relations jointly<br>
</li>
<li>Impose a <strong>rank restriction</strong> on the long-run matrix $ = ’ $<br>
</li>
<li>Solve a <strong>reduced-rank likelihood problem</strong> to identify the cointegration space <span class="math inline">\(\mathrm{span}(\beta)\)</span><br>
</li>
<li>Use the resulting eigenvalues to <strong>test and select the cointegration rank</strong> <span class="math inline">\(r\)</span><br>
</li>
<li>Given <span class="math inline">\(\widehat{\beta}\)</span>, recover the adjustment coefficients <span class="math inline">\(\widehat{\alpha}\)</span> by least squares</li>
</ul>
<p><br></p>
<p><strong>Choosing the cointegration rank (<span class="math inline">\(r\)</span>): practical guidance</strong></p>
<ul>
<li><p><strong>Trace test:</strong> asks <em>“Is there evidence for any additional long-run relations beyond <span class="math inline">\(r\)</span>?”</em><br>
→ joint test, often selects <strong>larger <span class="math inline">\(r\)</span></strong></p></li>
<li><p><strong>Max-eigen test:</strong> asks <em>“Is there evidence for exactly one more relation?”</em><br>
→ sequential, easier to interpret step-by-step, often selects <strong>smaller <span class="math inline">\(r\)</span></strong></p></li>
</ul>
<p><br></p>
<p><strong>Result:</strong> the procedure jointly determines the cointegration rank <span class="math inline">\(r\)</span> and the long-run relations <span class="math inline">\(\beta\)</span>, with <span class="math inline">\(\alpha\)</span> obtained conditional on <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="johansen-vs.-englegranger-what-is-different" class="level2">
<h2 class="anchored" data-anchor-id="johansen-vs.-englegranger-what-is-different">Johansen vs.&nbsp;Engle–Granger (What Is Different?)</h2>
<p><strong>Engle–Granger (single-equation)</strong></p>
<ul>
<li>estimates one cointegrating relation by OLS in <span class="math inline">\(y_t = \theta' z_t + e_t\)</span></li>
<li>then estimates an ECM using <span class="math inline">\(\widehat e_{t-1}\)</span> as a regressor</li>
<li>natural when <span class="math inline">\(r=1\)</span> and you are willing to <strong>choose</strong> the “dependent” variable</li>
<li>cannot handle <span class="math inline">\(r&gt;1\)</span>; results depend on normalization / equation choice</li>
</ul>
<p><br></p>
<p><strong>Johansen (system VECM)</strong></p>
<ul>
<li>treats the VECM as a <strong>system</strong> and estimates the cointegration space <span class="math inline">\(\mathrm{span}(\beta)\)</span></li>
<li>allows <strong>multiple</strong> long-run relations (<span class="math inline">\(r=0,1,\dots,K-1\)</span>)</li>
<li>provides <strong>likelihood-based rank tests</strong> (trace / max-eigen) for selecting <span class="math inline">\(r\)</span></li>
<li>identifies <span class="math inline">\(\beta\)</span> only up to nonsingular rotations; a normalization is imposed for interpretation</li>
</ul>
<p><br></p>
<p><strong>Common practical inputs:</strong> lag length <span class="math inline">\(p\)</span> and deterministic terms (constant/trend) must be specified.</p>
</section>
<section id="model-selection-and-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="model-selection-and-diagnostics">Model Selection and Diagnostics</h2>
<p>In empirical work, model selection tools are used to choose a <strong>candidate specification</strong>.</p>
<p>Common examples include:</p>
<ul>
<li>Akaike Information Criterion (AIC)</li>
<li>Bayesian Information Criterion (BIC)</li>
<li>Hannan–Quinn Criterion (HQ)</li>
</ul>
<p><br></p>
<p>Once a model is selected, diagnostics are used to assess whether the chosen specification is <strong>adequate</strong>.</p>
<p>Typical diagnostics include:</p>
<ul>
<li>residual autocorrelation checks<br>
</li>
<li>stability checks<br>
</li>
<li>basic distributional diagnostics</li>
</ul>
</section>
<section id="testing-adjustment-in-a-vecm-restrictions-on-alpha" class="level2">
<h2 class="anchored" data-anchor-id="testing-adjustment-in-a-vecm-restrictions-on-alpha">Testing Adjustment in a VECM: Restrictions on <span class="math inline">\(\alpha\)</span></h2>
<p>In the VECM <span class="math display">\[
\Delta x_t = \alpha\beta' x_{t-1} + \cdots + u_t,
\]</span> <span class="math inline">\(\alpha\in\mathbb{R}^{K\times r}\)</span> collects <strong>adjustment coefficients</strong>: row <span class="math inline">\(i\)</span> describes how <span class="math inline">\(\Delta x_{i,t}\)</span> responds to the <span class="math inline">\(r\)</span> equilibrium errors.</p>
<p><br></p>
<section id="general-form-linear-restrictions" class="level3">
<h3 class="anchored" data-anchor-id="general-form-linear-restrictions">General Form: Linear Restrictions</h3>
<p>Hypotheses about adjustment are typically written as <span class="math display">\[
H_0:\ R\,\mathrm{vec}(\alpha)=q,
\]</span> where <span class="math inline">\(R\)</span> selects or combines elements of <span class="math inline">\(\alpha\)</span>.</p>
<p>Examples:</p>
<ul>
<li><strong>No adjustment for variable <span class="math inline">\(i\)</span></strong> (weak exogeneity for <span class="math inline">\(\beta\)</span>): <span class="math display">\[
H_0:\ \alpha_{i\cdot}=0 \quad (r\ \text{restrictions})
\]</span></li>
<li><strong>Only relation 1 adjusts variable <span class="math inline">\(i\)</span></strong>: <span class="math display">\[
H_0:\ \alpha_{i2}=\cdots=\alpha_{ir}=0
\]</span></li>
<li><strong>Equal adjustment speeds across variables</strong> (example): <span class="math display">\[
H_0:\ \alpha_{1\cdot}=\alpha_{2\cdot}
\]</span></li>
</ul>
<p><br></p>
</section>
<section id="inference-1" class="level3">
<h3 class="anchored" data-anchor-id="inference-1">Inference</h3>
<p>Under standard VECM regularity conditions (given the cointegration rank <span class="math inline">\(r\)</span>):</p>
<ul>
<li>LR / Wald / LM tests for linear restrictions on <span class="math inline">\(\alpha\)</span></li>
<li>asymptotic <span class="math inline">\(\chi^2\)</span> distribution (df = number of restrictions)</li>
<li>implemented in standard VECM software</li>
</ul>
</section>
</section>
<section id="example-a-small-open-economy" class="level2">
<h2 class="anchored" data-anchor-id="example-a-small-open-economy">Example: A Small Open Economy</h2>
<p>Consider a small open economy with <span class="math display">\[
x_t =
\begin{bmatrix}
y_t \\ p_t \\ p_t^{\ast}
\end{bmatrix},
\]</span> where <span class="math inline">\(y_t\)</span> is domestic output, <span class="math inline">\(p_t\)</span> is the domestic price level, and <span class="math inline">\(p_t^{\ast}\)</span> is the foreign price level.</p>
<p>Suppose theory suggests a PPP-type long-run relation: <span class="math display">\[
\beta' x_t = p_t - p_t^{\ast}
\quad \text{is stationary}.
\]</span></p>
<p>Then the VECM implies <span class="math display">\[
\Delta x_t
=
\alpha (p_{t-1}-p_{t-1}^{\ast})
+\sum_{i=1}^{p-1}\Gamma_i \Delta x_{t-i}
+u_t,
\qquad
\alpha =
\begin{bmatrix}
\alpha_y\\ \alpha_p\\ \alpha_{p^\ast}
\end{bmatrix}.
\]</span></p>
<p><br></p>
<div class="fragment">
<section id="a-meaningful-restriction" class="level3">
<h3 class="anchored" data-anchor-id="a-meaningful-restriction">A meaningful restriction</h3>
<p>Small open economy assumption: foreign prices do <strong>not</strong> adjust to domestic disequilibrium: <span class="math display">\[
H_0:\ \alpha_{p^\ast}=0.
\]</span></p>
<p>Interpretation:</p>
<ul>
<li>deviations from PPP are corrected through domestic adjustment</li>
<li><span class="math inline">\(p_t^{\ast}\)</span> is <strong>weakly exogenous</strong> for the cointegrating relation <span class="math inline">\(\beta\)</span></li>
</ul>
</section>
</div>
</section>
<section id="summary-what-vars-and-vecms-do-and-do-not-do" class="level2">
<h2 class="anchored" data-anchor-id="summary-what-vars-and-vecms-do-and-do-not-do">Summary: What VARs and VECMs Do — and Do Not Do</h2>
<p>In this unit, we studied VARs and VECMs as <strong>reduced-form representations</strong> of multivariate time series.</p>
<p>They allow us to:</p>
<ul>
<li>model joint dynamics without imposing exogeneity</li>
<li>estimate long-run relations via cointegration</li>
<li>produce forecasts and forecast uncertainty</li>
</ul>
<p>However, the shocks <span class="math inline">\(u_t\)</span> in a VAR or VECM are:</p>
<ul>
<li>statistical innovations</li>
<li>generally correlated across equations</li>
<li>not economically interpretable by construction</li>
</ul>
<p><br></p>
<p><strong>Next unit:</strong> how additional identifying assumptions transform reduced-form innovations into <strong>structural shocks</strong> with economic meaning.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>